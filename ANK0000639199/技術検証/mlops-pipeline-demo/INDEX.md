# MLOps Pipeline Demo - Complete Index

**Project Location**: `c:/Users/matty/coachtech/mlops-pipeline-demo`
**Created**: 2026-01-10
**Status**: âœ… Ready to Use

## ğŸ“š Documentation Files

### Start Here
1. **[QUICK_START.md](QUICK_START.md)** - Get started in 5 minutes
   - Prerequisites checklist
   - Two ways to run (automated vs manual)
   - Common issues and solutions
   - Quick commands reference

2. **[README.md](README.md)** - Main documentation (8,837 bytes)
   - Complete project overview
   - Detailed setup instructions
   - Usage examples
   - Learning resources
   - Troubleshooting guide

### Reference Documentation
3. **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** - Project overview
   - What's included
   - File structure
   - Technology stack
   - Customization guide
   - Performance benchmarks

4. **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture
   - Component details
   - Data flow diagrams
   - Technology stack
   - Scalability considerations
   - Security best practices

5. **[WORKFLOW.md](WORKFLOW.md)** - Detailed workflow
   - Complete workflow diagram
   - Step-by-step execution
   - Data flow visualization
   - Success metrics

6. **[INDEX.md](INDEX.md)** - This file
   - Complete file index
   - Quick navigation
   - File purposes

## ğŸ”§ Configuration Files

### Docker & Infrastructure
- **[docker-compose.yml](docker-compose.yml)** - Service orchestration
  - MLflow server configuration
  - Jupyter Lab setup
  - Pipeline container definition
  - Network and volume configuration

- **[Dockerfile](Dockerfile)** - Pipeline container image
  - Python 3.10 base image
  - Dependency installation
  - Working directory setup
  - Environment variables

- **[.gitignore](.gitignore)** - Git ignore rules
  - Python artifacts
  - Generated models
  - MLflow runs
  - Jupyter checkpoints
  - Docker files

## ğŸ Python Pipeline Files

### Core Pipeline Scripts
Located in `pipeline/` directory:

1. **[pipeline/preprocess.py](pipeline/preprocess.py)** - Data preprocessing (6,523 bytes)
   - `load_data()` - Load CSV data
   - `validate_data()` - Data quality checks
   - `clean_data()` - Handle missing values and outliers
   - `encode_features()` - Categorical encoding and scaling
   - `split_data()` - Train/test split
   - `save_processed_data()` - Save processed datasets
   - `preprocess_data()` - Main pipeline function

2. **[pipeline/train.py](pipeline/train.py)** - Model training (8,964 bytes)
   - `parse_args()` - Command-line argument parsing
   - `load_processed_data()` - Load preprocessed data
   - `train_model()` - Train Random Forest classifier
   - `evaluate_model()` - Calculate metrics
   - `get_feature_importance()` - Feature importance analysis
   - `save_model()` - Save trained model
   - `main()` - Main training pipeline with MLflow logging

3. **[pipeline/evaluate.py](pipeline/evaluate.py)** - Model evaluation (11,234 bytes)
   - `parse_args()` - Command-line argument parsing
   - `load_model_and_data()` - Load model and test data
   - `generate_confusion_matrix()` - Create confusion matrix plot
   - `generate_roc_curve()` - Create ROC curve with AUC
   - `generate_precision_recall_curve()` - Create PR curve
   - `generate_classification_report()` - Detailed metrics
   - `calculate_business_metrics()` - Business-relevant metrics
   - `main()` - Main evaluation pipeline

4. **[pipeline/requirements.txt](pipeline/requirements.txt)** - Python dependencies
   - Core ML libraries (numpy, pandas, scikit-learn)
   - MLflow for experiment tracking
   - Visualization libraries (matplotlib, seaborn)
   - Jupyter and notebook tools
   - Utility libraries

## ğŸ“Š Data Files

### Sample Data
- **[data/sample_data.csv](data/sample_data.csv)** - Sample dataset (4,892 bytes)
  - 100 customer records
  - 8 columns: CustomerID, Age, Tenure, MonthlyCharges, TotalCharges, Contract, PaymentMethod, Churn
  - Binary classification target (Churn: 0 or 1)

### Generated Data (Created During Pipeline Execution)
- `data/processed/X_train.csv` - Training features
- `data/processed/X_test.csv` - Test features
- `data/processed/y_train.csv` - Training labels
- `data/processed/y_test.csv` - Test labels
- `data/processed/encoders.pkl` - Label encoders
- `data/processed/scaler.pkl` - Feature scaler

## ğŸ¤– Model Files

### Generated Models (Created During Training)
- `models/churn_model.pkl` - Trained Random Forest model
- `models/feature_importance.csv` - Feature importance scores

## ğŸ“ˆ Metrics Files

### Generated Metrics (Created During Evaluation)
- `metrics/confusion_matrix.png` - Confusion matrix visualization
- `metrics/roc_curve.png` - ROC curve with AUC score
- `metrics/precision_recall_curve.png` - Precision-recall curve
- `metrics/classification_report.json` - Detailed classification metrics
- `metrics/business_metrics.json` - Business-relevant metrics

## ğŸ““ Jupyter Notebooks

- **[notebooks/exploration.ipynb](notebooks/exploration.ipynb)** - Data exploration notebook
  - Setup and imports
  - Data loading and exploration
  - Exploratory data analysis
  - Model evaluation (if model exists)
  - MLflow experiment viewing
  - Next steps and suggestions

## ğŸš€ Automation Scripts

### Windows Scripts
1. **[run-pipeline.bat](run-pipeline.bat)** - Automated pipeline runner
   - Starts Docker services
   - Runs preprocessing
   - Trains model
   - Evaluates model
   - Displays results

2. **[test-setup.bat](test-setup.bat)** - Setup verification
   - Checks Docker installation
   - Verifies project structure
   - Tests basic functionality
   - Provides troubleshooting tips

### Linux/Mac Scripts
1. **[run-pipeline.sh](run-pipeline.sh)** - Automated pipeline runner
   - Colored output
   - Service health checks
   - Complete pipeline execution
   - Result summary

2. **[test-setup.sh](test-setup.sh)** - Setup verification
   - Comprehensive tests
   - Docker build verification
   - Service startup tests
   - Dependency checks

## ğŸ—‚ï¸ Directory Structure

```
mlops-pipeline-demo/
â”œâ”€â”€ ğŸ“„ Documentation (7 files)
â”‚   â”œâ”€â”€ README.md              (Main documentation)
â”‚   â”œâ”€â”€ QUICK_START.md         (Quick start guide)
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md     (Project overview)
â”‚   â”œâ”€â”€ ARCHITECTURE.md        (System architecture)
â”‚   â”œâ”€â”€ WORKFLOW.md            (Workflow guide)
â”‚   â”œâ”€â”€ INDEX.md               (This file)
â”‚   â””â”€â”€ .gitignore             (Git ignore rules)
â”‚
â”œâ”€â”€ ğŸ³ Docker (2 files)
â”‚   â”œâ”€â”€ docker-compose.yml     (Service orchestration)
â”‚   â””â”€â”€ Dockerfile             (Pipeline image)
â”‚
â”œâ”€â”€ ğŸ¤– Pipeline (4 files)
â”‚   â”œâ”€â”€ preprocess.py          (Data preprocessing)
â”‚   â”œâ”€â”€ train.py               (Model training)
â”‚   â”œâ”€â”€ evaluate.py            (Model evaluation)
â”‚   â””â”€â”€ requirements.txt       (Dependencies)
â”‚
â”œâ”€â”€ ğŸ“Š Data (1 file + generated)
â”‚   â”œâ”€â”€ sample_data.csv        (Sample dataset)
â”‚   â””â”€â”€ processed/             (Generated data)
â”‚
â”œâ”€â”€ ğŸ¯ Models (generated)
â”‚   â””â”€â”€ *.pkl                  (Trained models)
â”‚
â”œâ”€â”€ ğŸ“ˆ Metrics (generated)
â”‚   â””â”€â”€ *.png, *.json          (Evaluation results)
â”‚
â”œâ”€â”€ ğŸ““ Notebooks (1 file)
â”‚   â””â”€â”€ exploration.ipynb      (Jupyter notebook)
â”‚
â””â”€â”€ ğŸš€ Scripts (4 files)
    â”œâ”€â”€ run-pipeline.bat       (Windows runner)
    â”œâ”€â”€ run-pipeline.sh        (Linux/Mac runner)
    â”œâ”€â”€ test-setup.bat         (Windows setup test)
    â””â”€â”€ test-setup.sh          (Linux/Mac setup test)
```

## ğŸ“– Reading Order

### For First-Time Users
1. Start with **QUICK_START.md** to get running quickly
2. Run **test-setup.bat** (or .sh) to verify installation
3. Run **run-pipeline.bat** (or .sh) to execute the pipeline
4. View **README.md** for detailed documentation
5. Open Jupyter notebook for interactive exploration
6. Check **MLflow UI** (http://localhost:5000) for results

### For Understanding the System
1. Read **PROJECT_SUMMARY.md** for overview
2. Study **ARCHITECTURE.md** for system design
3. Follow **WORKFLOW.md** for execution flow
4. Review pipeline scripts for implementation details

### For Developers
1. Read **ARCHITECTURE.md** for system architecture
2. Study pipeline scripts (preprocess.py, train.py, evaluate.py)
3. Review **docker-compose.yml** for service configuration
4. Check **requirements.txt** for dependencies
5. Modify and experiment with the code

## ğŸ”— Quick Links

### Services (when running)
- **MLflow UI**: http://localhost:5000
- **Jupyter Lab**: http://localhost:8888 (token: mlops-demo)

### Key Commands
```bash
# Start services
docker-compose up -d

# Run pipeline (automated)
./run-pipeline.sh          # Linux/Mac
run-pipeline.bat           # Windows

# Run pipeline (manual)
docker-compose run --rm pipeline python pipeline/train.py
docker-compose run --rm pipeline python pipeline/evaluate.py

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

## ğŸ“Š File Statistics

### Total Files
- Documentation: 7 files
- Configuration: 3 files
- Python scripts: 4 files
- Data files: 1 sample file
- Notebooks: 1 file
- Automation: 4 scripts
- **Total: 20 files**

### File Sizes
- Documentation: ~50 KB
- Python code: ~26 KB
- Sample data: ~5 KB
- Configuration: ~3 KB
- **Total: ~84 KB** (excluding generated files)

### Lines of Code
- Python pipeline: ~800 lines
- Documentation: ~1,500 lines
- Jupyter notebook: ~200 cells
- **Total: ~2,500+ lines**

## ğŸ¯ Usage Patterns

### Quick Demo
```bash
cd c:/Users/matty/coachtech/mlops-pipeline-demo
run-pipeline.bat
```

### Development Cycle
```bash
# 1. Start environment
docker-compose up -d

# 2. Develop in Jupyter
# Open http://localhost:8888

# 3. Test changes
docker-compose run --rm pipeline python pipeline/train.py

# 4. View results
# Open http://localhost:5000

# 5. Iterate
# Repeat 2-4

# 6. Stop
docker-compose down
```

### Experimentation
```bash
# Try different hyperparameters
docker-compose run --rm pipeline python pipeline/train.py \
  --n-estimators 200 --max-depth 15

# View and compare in MLflow UI
open http://localhost:5000
```

## ğŸ” Finding What You Need

### Need to...
- **Get started quickly?** â†’ QUICK_START.md
- **Understand the system?** â†’ ARCHITECTURE.md
- **See the workflow?** â†’ WORKFLOW.md
- **Modify preprocessing?** â†’ pipeline/preprocess.py
- **Change the model?** â†’ pipeline/train.py
- **Add metrics?** â†’ pipeline/evaluate.py
- **Add services?** â†’ docker-compose.yml
- **Explore data?** â†’ notebooks/exploration.ipynb
- **Troubleshoot?** â†’ README.md (Troubleshooting section)

## âœ… Verification Checklist

- [ ] All documentation files present (7 files)
- [ ] Docker configuration files present (2 files)
- [ ] Pipeline scripts present (4 files)
- [ ] Sample data file present (1 file)
- [ ] Jupyter notebook present (1 file)
- [ ] Automation scripts present (4 files)
- [ ] Total: 21 files (including this INDEX.md)

## ğŸ“ Learning Path

1. **Beginner**: Follow QUICK_START.md
2. **Intermediate**: Study pipeline scripts
3. **Advanced**: Modify and extend the system
4. **Expert**: Deploy to production environment

## ğŸ“ Support

- Check **README.md** for detailed instructions
- Review **QUICK_START.md** for common issues
- Run **test-setup.bat** to verify installation
- Check Docker logs: `docker-compose logs`

---

**Last Updated**: 2026-01-10
**Project Version**: 1.0
**Status**: Production Ready âœ…

**Ready to start?** Run `test-setup.bat` then `run-pipeline.bat`!
